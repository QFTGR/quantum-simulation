{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning for Hamiltonian engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, time, signal\n",
    "import gym\n",
    "from gym import error, spaces\n",
    "from gym import utils\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "from scipy import sparse\n",
    "import gym_pp\n",
    "\n",
    "# 10-base to arbitrary base\n",
    "def dec2base(num,base,length):\n",
    "    s=''\n",
    "    if num>base**length-1:\n",
    "        raise ValueError('Input number exceeds the maximum number allowed by the length')\n",
    "    for i in range(length):\n",
    "        s=s+chr(ord('0')+int(num/(base**(length-1-i))))\n",
    "        num=num-int(num/(base**(length-1-i)))*(base**(length-1-i))\n",
    "    return s\n",
    "\n",
    "# given a state dict, return a str that specifies all pulses\n",
    "def getPPstr(state,actionDict=['d','x','y','-x','-y']):\n",
    "    num=state['pp']\n",
    "    base=len(actionDict)\n",
    "    length=state['n']\n",
    "    rawPulseStr=dec2base(num,base,length)\n",
    "    pulseStr=''\n",
    "    for p in rawPulseStr:\n",
    "        pulseStr=actionDict[int(p)]+','+pulseStr\n",
    "    pulseStr=pulseStr[0:-1]\n",
    "    return pulseStr\n",
    "\n",
    "env = gym.make('pp-v0')\n",
    "\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pauli(n):\n",
    "    if n==0:\n",
    "      return np.eye(2)\n",
    "    elif n==1:\n",
    "      return np.array([[0,1],[1,0]])\n",
    "    elif n==2:\n",
    "      return np.array([[0,-1j],[1j,0]])\n",
    "    elif n==3:\n",
    "      return np.array([[1,0],[0,-1]])\n",
    "    else:\n",
    "      raise ValueError('Input must be integer from 0 to 3.')\n",
    "\n",
    "# returns sigma_a^p*sigma_b^q, with a,b = 1,2,3, p,q being position\n",
    "def Kron2body(N_atom,a,b,p,q):\n",
    "    y=1\n",
    "    for i in range(N_atom):\n",
    "        if i==p:\n",
    "            y=np.kron(y,Pauli(a))\n",
    "        elif i==q:\n",
    "            y=np.kron(y,Pauli(b))\n",
    "        else:\n",
    "            y=np.kron(y,np.eye(2))\n",
    "    return y\n",
    "\n",
    "def Hamiltonian(N_atom,bc,cplist,model):\n",
    "    H=np.zeros((2**N_atom,2**N_atom))\n",
    "    for pp in range(len(cplist)):\n",
    "        for p in range(N_atom):\n",
    "            if bc=='p':\n",
    "                q=(p+pp+1)%N_atom\n",
    "            elif bc=='o':\n",
    "                q=p+pp+1\n",
    "                if q>=N_atom:\n",
    "                    continue\n",
    "            H=H+cplist[pp]*(model[0]*Kron2body(N_atom,1,1,p,q)\n",
    "                            +model[1]*Kron2body(N_atom,2,2,p,q)\n",
    "                            +model[2]*Kron2body(N_atom,3,3,p,q))\n",
    "    if np.max(np.abs(np.imag(H)))<1e-10:\n",
    "        H=np.real(H)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTime=20\n",
    "nSpin=3\n",
    "env.setParam(maxTime,nSpin)\n",
    "\n",
    "env.setTarget(np.eye(2**env.nSpin))\n",
    "\n",
    "H=Hamiltonian(nSpin,'p',[1],[-0.5,-0.5,1])\n",
    "J=8.18e-3\n",
    "t=1\n",
    "env.setU0(expm(-1j*J*H*t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-01273415b290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# qTable=np.zeros((5**maxTime,6))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mqTable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmaxTime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# how many actions are available (maxTime is not considered)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nEpisodes=1000\n",
    "alpha=0.1 # gradient step\n",
    "gamma=0.9 # discount factor\n",
    "# beta=0.2*np.ones(nEpisodes) # inverse temperature\n",
    "beta=np.logspace(-3.0, -.5, num=nEpisodes) # inverse temperature\n",
    "\n",
    "# qTable=np.zeros((5**maxTime,6))\n",
    "qTable=sparse.csr_matrix((5**maxTime,6))\n",
    "\n",
    "# how many actions are available (maxTime is not considered)\n",
    "def getAvailableAction(state,frame):\n",
    "    if np.max(np.abs(frame-np.eye(2)))<1e-10 and state['n']!=0:\n",
    "        return 6\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "# get the next action\n",
    "def getAction(state,frame,beta):\n",
    "    if state['n']==maxTime:\n",
    "        return 5\n",
    "    nAction=getAvailableAction(state,frame)\n",
    "    prob=np.exp(beta*qTable[state['pp'],0:nAction])\n",
    "    prob=prob/np.sum(prob)\n",
    "    action=np.random.choice(nAction,1,p=prob)\n",
    "    return action[0]\n",
    "\n",
    "best_reward=0\n",
    "best_pp=None\n",
    "reward_list=[]\n",
    "for episode in range(nEpisodes):\n",
    "    state,info=env.reset()\n",
    "    frame=info[\"frame\"]\n",
    "    done=False\n",
    "    while not done:\n",
    "        action=getAction(state,frame,beta[episode])\n",
    "        next_state, reward, done, info=env.step(action)\n",
    "        frame=info[\"frame\"]\n",
    "        if done:\n",
    "            qTable[state['pp'],action]=(1-alpha)*qTable[state['pp'],action]+alpha*reward\n",
    "        else:\n",
    "            qTable[state['pp'],action]=((1-alpha)*qTable[state['pp'],action]+\n",
    "                                    alpha*(reward+gamma*np.max(qTable[next_state['pp'], 0:getAvailableAction(next_state,frame)])))\n",
    "            state=next_state\n",
    "    if reward>best_reward:\n",
    "        best_reward=reward\n",
    "        best_pp=getPPstr(env.state)\n",
    "        print('Episode: '+str(episode))\n",
    "        print(getPPstr(env.state))\n",
    "        print('Fidelity: '+str(env.getFidelity())+', Reward: '+str(reward))\n",
    "    reward_list.append(reward)\n",
    "    if (episode+1)%1000==0 and episode!=0:\n",
    "        recent_reward=reward_list[episode-99:episode+1]\n",
    "        print('Recent 1000 average reward: '+str(np.mean(recent_reward)))\n",
    "        \n",
    "print('Best sequece: '+best_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "H=np.array([[3,0,0,0,0,0,0,0],\n",
    "     [0,-1,-1,0,-1,0,0,0],\n",
    "     [0,-1,-1,0,-1,0,0,0],\n",
    "     [0,0,0,-1,0,-1,-1,0],\n",
    "     [0,-1,-1,0,-1,0,0,0],\n",
    "     [0,0,0,-1,0,-1,-1,0],\n",
    "     [0,0,0,-1,0,-1,-1,0],\n",
    "     [0,0,0,0,0,0,0,3]])\n",
    "J=8.18e-3\n",
    "t=1\n",
    "env.setU0(expm(-1j*J*H*t*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-x,-y,y,x,y,-y\n",
      "[[1.00000000e+00+0.j 2.36158002e-17+0.j]\n",
      " [2.36158002e-17+0.j 1.00000000e+00+0.j]]\n",
      "Fidelity: 0.9999999938473954, Reward: 18.906390324790543\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "observation, reward, done, info = env.step(3)\n",
    "\n",
    "observation, reward, done, info = env.step(4)\n",
    "\n",
    "observation, reward, done, info = env.step(2)\n",
    "\n",
    "observation, reward, done, info = env.step(1)\n",
    "\n",
    "observation, reward, done, info = env.step(2)\n",
    "\n",
    "observation, reward, done, info = env.step(4)\n",
    "\n",
    "observation, reward, done, info = env.step(5)\n",
    "print(getPPstr(env.state))\n",
    "print(env.frame)\n",
    "print('Fidelity: '+str(env.getFidelity())+', Reward: '+str(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d,x,-y,d,y,-x\n",
      "[[ 1.00000000e+00+0.j -3.25176795e-17+0.j]\n",
      " [-3.25176795e-17+0.j  1.00000000e+00+0.j]]\n",
      "Fidelity: 0.9999999999964451, Reward: 26.362684583126395\n"
     ]
    }
   ],
   "source": [
    "# WAHUHA\n",
    "env.reset()\n",
    "observation, reward, done, info = env.step(0)\n",
    "\n",
    "observation, reward, done, info = env.step(1)\n",
    "\n",
    "observation, reward, done, info = env.step(4)\n",
    "\n",
    "observation, reward, done, info = env.step(0)\n",
    "\n",
    "observation, reward, done, info = env.step(2)\n",
    "\n",
    "observation, reward, done, info = env.step(3)\n",
    "\n",
    "observation, reward, done, info = env.step(5)\n",
    "\n",
    "print('WAHUHA: '+getPPstr(env.state))\n",
    "print(env.frame)\n",
    "print('Fidelity: '+str(env.getFidelity())+', Reward: '+str(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
